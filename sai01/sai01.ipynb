{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43172da",
   "metadata": {},
   "source": [
    "# SAI #01: Column Based vs. Row Based Storage, Kafka - Writing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6368d235",
   "metadata": {},
   "source": [
    "## 1.ð—¥ð—¼ð˜„ ð—•ð—®ð˜€ð—²ð—± ð˜ƒð˜€ ð—–ð—¼ð—¹ð˜‚ð—ºð—» ð—•ð—®ð˜€ð—²ð—± ð—™ð—¶ð—¹ð—² ð—™ð—¼ð—¿ð—ºð—®ð˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae027f57",
   "metadata": {},
   "source": [
    "### 1.1. ð—¥ð—¼ð˜„ ð—•ð—®ð˜€ð—²ð—±: Use Avro for example file format\n",
    "Ref: https://avro.apache.org/docs/1.11.1/getting-started-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c226b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avro\n",
    "import json\n",
    "\n",
    "from avro.datafile import DataFileWriter, DataFileReader\n",
    "from avro.io import DatumWriter, DatumReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43520d",
   "metadata": {},
   "source": [
    "+ **Step1: Prepare the schema of avro .avsc file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f5be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_schema = {\n",
    "    \"namespace\": \"Company\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"Age\", \"type\": \"int\"},\n",
    "        {\"name\": \"Occupation\", \"type\": \"string\"},\n",
    "        {\"name\": \"No_of_dog\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\n",
    "# schema = avro.schema.parse(open(\"user.avsc\", \"rb\").read())\n",
    "schema = avro.schema.parse(json.dumps(user_schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca2dc4",
   "metadata": {},
   "source": [
    "+ **Step2: Write file to .avsc file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "731f7526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_data = [\n",
    "    {\"name\": \"Aurimas\", \"Age\": 31, \"Occupation\": \"MLOps\", \"No_of_dog\":2},\n",
    "    {\"name\": \"Thomas\", \"Age\": 25, \"Occupation\": \"DE\", \"No_of_dog\":0},\n",
    "    {\"name\": \"Suzan\", \"Age\": 29, \"Occupation\": \"MLE\", \"No_of_dog\":1},\n",
    "    {\"name\": \"Peter\", \"Age\": 34, \"Occupation\": \"SWE\", \"No_of_dog\":0}\n",
    "]\n",
    "writer = DataFileWriter(open(\"user.avsc\", \"wb\"), DatumWriter(), schema)\n",
    "# Write row by row\n",
    "for ele in row_data:\n",
    "    writer.append(ele)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93d722",
   "metadata": {},
   "source": [
    "+ **Step3: Reading file from .avsc file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b378a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Aurimas', 'Age': 31, 'Occupation': 'MLOps', 'No_of_dog': 2}\n",
      "{'name': 'Thomas', 'Age': 25, 'Occupation': 'DE', 'No_of_dog': 0}\n",
      "{'name': 'Suzan', 'Age': 29, 'Occupation': 'MLE', 'No_of_dog': 1}\n",
      "{'name': 'Peter', 'Age': 34, 'Occupation': 'SWE', 'No_of_dog': 0}\n"
     ]
    }
   ],
   "source": [
    "reader = DataFileReader(open(\"user.avsc\", \"rb\"), DatumReader())\n",
    "for user in reader:\n",
    "    print(user)\n",
    "reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08b587",
   "metadata": {},
   "source": [
    "+ **Step4: Add more data to exist .avsc file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed208d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = DataFileWriter(open(\"user.avsc\", \"ab+\"), DatumWriter())\n",
    "writer.append({\"name\": \"David\", \"Age\": 28, \"Occupation\": \"SE\", \"No_of_dog\":3})\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28186ac",
   "metadata": {},
   "source": [
    "### 1.2. Column Based: Use Parquet, ORC file for example file formats\n",
    "\n",
    "Ref: https://arrow.apache.org/docs/python/parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86f116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde2348",
   "metadata": {},
   "source": [
    "+ **Step1: Prepare the column based data as Json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3cc8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_data = {\n",
    "    \"name\": [\"Aurimas\", \"Thomas\", \"Suzan\", \"Peter\"],\n",
    "    \"Age\": [31, 25, 29, 34],\n",
    "    \"Occupation\": [\"MLOps\", \"DE\", \"MLE\", \"SWE\"],\n",
    "    \"No_of_dog\": [2, 0, 1, 0]\n",
    "}\n",
    "df = pd.DataFrame(column_data)\n",
    "table = pa.Table.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63d1eb",
   "metadata": {},
   "source": [
    "+ **Step2: Write column data to the parquet file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f714140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the parquet file \n",
    "pq.write_table(table, 'user.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e342aec",
   "metadata": {},
   "source": [
    "+ **Step3: Read the data from parquet file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2f0ceef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "name: string\n",
       "Age: int64\n",
       "Occupation: string\n",
       "No_of_dog: int64\n",
       "----\n",
       "name: [[\"Aurimas\",\"Thomas\",\"Suzan\",\"Peter\"]]\n",
       "Age: [[31,25,29,34]]\n",
       "Occupation: [[\"MLOps\",\"DE\",\"MLE\",\"SWE\"]]\n",
       "No_of_dog: [[2,0,1,0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the parquet file \n",
    "data = pq.read_table('user.parquet')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8f053",
   "metadata": {},
   "source": [
    "**ORC: https://arrow.apache.org/docs/python/orc.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508bf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import orc \n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ceea51",
   "metadata": {},
   "source": [
    "+ **Step1: Prepare the column based data as Json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b5b41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.table({\n",
    "    \"name\": [\"Aurimas\", \"Thomas\", \"Suzan\", \"Peter\"],\n",
    "    \"Age\": [31, 25, 29, 34],\n",
    "    \"Occupation\": [\"MLOps\", \"DE\", \"MLE\", \"SWE\"],\n",
    "    \"No_of_dog\": [2, 0, 1, 0]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a757c2",
   "metadata": {},
   "source": [
    "+ **Step2: Write column data to ORC file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5701e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the orc file \n",
    "orc.write_table(table, 'user.orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184688fe",
   "metadata": {},
   "source": [
    "+ **Step3: Read data from the orc file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f84afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "name: string\n",
       "Age: int64\n",
       "Occupation: string\n",
       "No_of_dog: int64\n",
       "----\n",
       "name: [[\"Aurimas\",\"Thomas\",\"Suzan\",\"Peter\"]]\n",
       "Age: [[31,25,29,34]]\n",
       "Occupation: [[\"MLOps\",\"DE\",\"MLE\",\"SWE\"]]\n",
       "No_of_dog: [[2,0,1,0]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from the orc file \n",
    "orc.read_table('user.orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f602002",
   "metadata": {},
   "source": [
    "## 2. Kafka: Producer and Consumer Examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a769a1fd",
   "metadata": {},
   "source": [
    "+ **Step1: Start the Kafka service to create the `my_topic` kafka connection**\n",
    "\n",
    "\n",
    "```bash\n",
    "# From the /sai01 folder, run the command below\n",
    "$ docker-compose up\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a5305",
   "metadata": {},
   "source": [
    "+ **Step2: Start the Kafka client to alway listen the `my_topic` kafka connection**\n",
    "\n",
    "\n",
    "```bash\n",
    "# From the /sai01 folder, run the command below\n",
    "(venv)$ python run_kafka_consumer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3e561",
   "metadata": {},
   "source": [
    "+ **Step3: Trigger the function below to run producer sending message to `my_topic` connection**\n",
    "\n",
    "When running the cell below at this step, it will send the message to `my_topic` kafka, and in the terminal log of step2's script, you could see the sending message like this \n",
    "```\n",
    "b'message 0'\n",
    "b'message 1'\n",
    "b'message 2'\n",
    "b'message 3'\n",
    "b'message 4'\n",
    "b'message 5'\n",
    "b'message 6'\n",
    "b'message 7'\n",
    "b'message 8'\n",
    "b'message 9'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81837b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Rerun this cell to resend message to kafka topic\n",
    "for i in range(10):\n",
    "    message = \"Test message {}\".format(i).encode('utf-8')\n",
    "    producer.send('my_topic', message)\n",
    "\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5beffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f7076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
